<div align="center">

# ğŸŒ Babelon ç¿»è­¯æœå‹™

**å¤šèªè¨€éŸ³é »è½‰éŒ„èˆ‡ç¿»è­¯å¹³å°**

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://python.org)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-green.svg)](https://fastapi.tiangolo.com)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue.svg)](https://docker.com)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

*æ•´åˆå…ˆé€² ASR èˆ‡ AI ç¿»è­¯æŠ€è¡“ï¼Œç‚ºæœƒè­°è¨˜éŒ„èˆ‡å³æ™‚ç¿»è­¯æä¾›å°ˆæ¥­è§£æ±ºæ–¹æ¡ˆ*

</div>

---

## ğŸ“ é …ç›®æ¦‚è¿°

**Babelon** æ˜¯ä¸€å€‹åŸºæ–¼ FastAPI çš„å¤šèªè¨€éŸ³é »è½‰éŒ„èˆ‡ç¿»è­¯æœå‹™å¹³å°ã€‚èåˆäº†æœ€æ–°çš„ ASRï¼ˆè‡ªå‹•èªéŸ³è­˜åˆ¥ï¼‰å’Œ AI ç¿»è­¯æŠ€è¡“ï¼Œæä¾›é«˜ç²¾åº¦çš„èªéŸ³è½‰æ–‡å­—å’Œå¤šèªè¨€ç¿»è­¯åŠŸèƒ½ã€‚

### ğŸ¯ é©ç”¨å ´æ™¯
- ğŸ“‹ **æœƒè­°è¨˜éŒ„** - è‡ªå‹•ç”Ÿæˆå¤šèªè¨€æœƒè­°ç´€éŒ„
- ğŸ™ï¸ **å³æ™‚èªéŸ³ç¿»è­¯** - ç·šä¸Šæœƒè­°å³æ™‚å¤šèªè¨€æ”¯æ´
- ğŸ“ **èªéŸ³å‚™å¿˜éŒ„** - å°‡èªéŸ³å¿«é€Ÿè½‰æ›ç‚ºå¯ç·¨è¼¯æ–‡å­—
- ğŸŒ **å¤šèªè¨€å…§å®¹å‰µä½œ** - ä¸€éµç”Ÿæˆå¤šèªè¨€ç‰ˆæœ¬å…§å®¹

## âœ¨ æ ¸å¿ƒç‰¹è‰²

<table>
<tr>
<td width="50%">

### ğŸ¯ **æ ¸å¿ƒåŠŸèƒ½**
- ğŸµ **é«˜ç²¾åº¦éŸ³é »è½‰éŒ„** - æ”¯æ´ Whisper large-v2/v3/turbo æ¨¡å‹
- ğŸŒ **å¤šèªè¨€ç¿»è­¯** - ä¸­æ–‡ï¼ˆç¹é«”ï¼‰ã€è‹±æ–‡ã€å¾·æ–‡äº’è­¯
- âš¡ **å³æ™‚ä¸²æµè™•ç†** - SSE æŠ€è¡“å¯¦ç¾å³æ™‚ç¿»è­¯å›é¥‹
- ğŸ”„ **å¤šé‡ç­–ç•¥è½‰éŒ„** - æœ€å¤š 4 ç¨®ç­–ç•¥ç¢ºä¿æœ€ä½³æº–ç¢ºåº¦
- ğŸ› ï¸ **æ™ºèƒ½å¾Œè™•ç†** - è‡ªå‹•ä¿®æ­£ ASR å¸¸è¦‹éŒ¯èª¤

</td>
<td width="50%">

### ğŸ›  **æŠ€è¡“äº®é»**
- ğŸš€ **å¤š AI æ¨¡å‹æ•´åˆ** - Whisperã€Gemmaã€Ollamaã€GPT-4o
- âš¡ **GPU åŠ é€Ÿé‹ç®—** - CUDA æ”¯æ´ï¼Œå¤§å¹…æå‡è™•ç†é€Ÿåº¦
- ğŸ³ **å®¹å™¨åŒ–éƒ¨ç½²** - Docker/Docker Compose ä¸€éµéƒ¨ç½²
- ğŸ”§ **æ™ºèƒ½è³‡æºç®¡ç†** - è‡ªå‹•æ¸…ç†ã€è¨˜æ†¶é«”å„ªåŒ–
- ğŸ›¡ï¸ **å¤šç­–ç•¥å®¹éŒ¯** - ç¢ºä¿æœå‹™ç©©å®šæ€§

</td>
</tr>
</table>

## ğŸ— ç³»çµ±æ¶æ§‹

## ğŸ— ç³»çµ±æ¶æ§‹

```
ğŸŒ Babelon ç¿»è­¯æœå‹™
â”‚
â”œâ”€â”€ ğŸš€ main.py                    # FastAPI ä¸»æ‡‰ç”¨ç¨‹å¼
â”‚
â”œâ”€â”€ ğŸ“‚ api/                       # API æ ¸å¿ƒæ¨¡çµ„
â”‚   â”œâ”€â”€ ğŸ§  model.py               # æ¨¡å‹ç®¡ç†ä¸­å¿ƒ
â”‚   â”œâ”€â”€ ğŸ”¤ gemma_translate.py     # Gemma ç¿»è­¯å¼•æ“
â”‚   â”œâ”€â”€ ğŸ’¬ gpt_translate.py       # GPT-4o ç¿»è­¯å¼•æ“
â”‚   â”œâ”€â”€ ğŸ¦™ ollama_translate.py    # Ollama ç¿»è­¯å¼•æ“
â”‚   â”œâ”€â”€ âš™ï¸ post_process.py        # æ™ºèƒ½å¾Œè™•ç†æ¨¡çµ„
â”‚   â””â”€â”€ ğŸ”’ threading_api.py       # å¤šåŸ·è¡Œç·’ API (æ©Ÿå¯†)
â”‚
â”œâ”€â”€ ğŸ“š lib/                       # å…±ç”¨å‡½å¼åº«
â”‚   â”œâ”€â”€ ğŸ—ï¸ base_object.py         # åŸºç¤ç‰©ä»¶å®šç¾©
â”‚   â”œâ”€â”€ âš™ï¸ constant.py            # ç³»çµ±å¸¸æ•¸è¨­å®š
â”‚   â””â”€â”€ â˜ï¸ azure_config.yaml      # Azure API è¨­å®š
â”‚
â”œâ”€â”€ ğŸ› ï¸ tools/                     # å·¥å…·ç¨‹å¼é›†
â”‚   â””â”€â”€ ğŸ”Š audio_splitter.py      # éŸ³é »åˆ†å‰²å·¥å…·
â”‚
â”œâ”€â”€ ğŸµ audio/                     # éŸ³é »æš«å­˜å€
â””â”€â”€ ğŸ“‹ logs/                      # ç³»çµ±æ—¥èªŒ
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ğŸ“‹ ç’°å¢ƒéœ€æ±‚

<table>
<tr>
<td><strong>ğŸ Python</strong></td>
<td>3.9 æˆ–ä»¥ä¸Šç‰ˆæœ¬</td>
</tr>
<tr>
<td><strong>ğŸ”¥ GPU</strong></td>
<td>NVIDIA CUDA æ”¯æ´ (å»ºè­° 4GB+ VRAM)</td>
</tr>
<tr>
<td><strong>ğŸ³ å®¹å™¨</strong></td>
<td>Docker & Docker Compose</td>
</tr>
<tr>
<td><strong>ğŸ’¾ è¨˜æ†¶é«”</strong></td>
<td>16GB+ RAM (å»ºè­°)</td>
</tr>
</table>

### ğŸ› ï¸ å®‰è£éƒ¨ç½²

<details>
<summary><strong>ğŸ³ æ–¹æ³•ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨è–¦ï¼‰</strong></summary>

```bash
# ğŸ“¥ è¤‡è£½é …ç›®
git clone https://github.com/bobo0303/Babelon-Translate-Service.git
cd Babelon-Translate-Service

# ğŸ—ï¸ å»ºç½®ä¸¦å•Ÿå‹•æœå‹™
docker build -t babelon .
# æˆ–ä½¿ç”¨ Docker Compose
docker-compose up -d

# ğŸ”§ é€²å…¥å®¹å™¨
docker exec -it babelon bash

# ğŸš€ å•Ÿå‹•æœå‹™
python main.py
```

</details>

<details>
<summary><strong>ğŸ’» æ–¹æ³•äºŒï¼šæœ¬åœ°å®‰è£</strong></summary>

```bash
# ğŸ“¦ å®‰è£ç›¸ä¾å¥—ä»¶
pip install -r requirements.txt

# âš™ï¸ è¨­å®šç’°å¢ƒè®Šæ•¸
export HUGGINGFACE_HUB_TOKEN="your-hf-token"

# ğŸš€ å•Ÿå‹•æœå‹™
python main.py
```

</details>

### âš™ï¸ åˆæ¬¡ä½¿ç”¨è¨­å®š

<table>
<tr>
<td>âš ï¸</td>
<td><strong>é‡è¦æé†’</strong>ï¼šä»¥ä¸‹æª”æ¡ˆéœ€è¦è‡ªè¡Œæº–å‚™ï¼ŒåŒ…å«æ©Ÿå¯†è³‡è¨ŠæœªåŒ…å«åœ¨ repository ä¸­</td>
</tr>
</table>

<details>
<summary><strong>ğŸ”§ æ­¥é©Ÿ 1ï¼šAzure OpenAI é…ç½®</strong></summary>

å»ºç«‹ `azure_config.yaml` æª”æ¡ˆä¸¦å¡«å…¥ä»¥ä¸‹å…§å®¹ï¼š

```yaml
API_KEY: "your_azure_api_key"
AZURE_API_VERSION: "xxxx-xx-xx-preview"
AZURE_ENDPOINT: "https://your-endpoint.openai.azure.com"
AZURE_DEPLOYMENT: "your-deployment-name"
```

</details>

<details>
<summary><strong>ğŸ¤— æ­¥é©Ÿ 2ï¼šHugging Face ç™»å…¥ï¼ˆGemma æ¨¡å‹ï¼‰</strong></summary>

```bash
# é¦–å…ˆéœ€è¦åœ¨ Hugging Face åŒæ„ä½¿ç”¨æ¢æ¬¾
# è¨ªå•ï¼šhttps://huggingface.co/google/gemma-3-4b-it

# ä½¿ç”¨ Token ç™»å…¥
huggingface-cli login --token your_hf_token
```

</details>

<details>
<summary><strong>ğŸ¦™ æ­¥é©Ÿ 3ï¼šOllama è¨­å®šï¼ˆå¯é¸ï¼‰</strong></summary>

```bash
# ğŸ³ å»ºç½® Ollama Docker å®¹å™¨
docker run -d -it --gpus all --shm-size 32G --runtime nvidia \
  --device=/dev/nvidia-uvm --device=/dev/nvidia-uvm-tools \
  --device=/dev/nvidiactl --device=/dev/nvidia0 \
  -v ./ollama:/root/.ollama -p 52013:11434 \
  --name ollama ollama/ollama

# ğŸ§ª æ¸¬è©¦ Ollama æ¨¡å‹
docker exec -it ollama ollama run gemma3:12b-it-qat --verbose
```

</details>

## ğŸ“‹ API æ–‡æª”

### åŸºæœ¬è³‡è¨Š
- **æœå‹™åœ°å€**ï¼š`http://localhost:80`
- **API æ–‡æª”**ï¼š`http://localhost:80/docs`
- **å¥åº·æª¢æŸ¥**ï¼š`GET /`

### ä¸»è¦ API ç«¯é»

#### ğŸµ éŸ³é »è½‰éŒ„ç¿»è­¯
**ç«¯é»**ï¼š`POST /translate`

å°‡éŸ³é »æª”æ¡ˆé€²è¡ŒèªéŸ³è­˜åˆ¥ä¸¦ç¿»è­¯æˆå¤šåœ‹èªè¨€ï¼Œæ”¯æ´æœƒè­°è¨˜éŒ„ã€èªéŸ³å‚™å¿˜éŒ„ç­‰æ‡‰ç”¨å ´æ™¯ã€‚

```http
POST /translate
Content-Type: multipart/form-data

file: audio file (.wav, .mp3, .m4a)
meeting_id: string
device_id: string  
audio_uid: string
times: datetime
o_lang: string (zh|en|de)
prev_text: string (å¯é¸ï¼Œå‰æ–‡èªå¢ƒ)
multi_strategy_transcription: int (1-4ï¼Œé è¨­1)
transcription_post_processing: bool (é è¨­true)
```

**å›æ‡‰æ ¼å¼**ï¼š
```json
{
  "status": "OK",
  "message": "ç¿»è­¯çµæœæ‘˜è¦",
  "data": {
    "meeting_id": "123",
    "device_id": "456", 
    "ori_lang": "zh",
    "text": {
      "zh": "ä¸­æ–‡ç¿»è­¯",
      "en": "English translation",
      "de": "Deutsche Ãœbersetzung"
    },
    "times": "2024-01-01T10:00:00",
    "audio_uid": "789",
    "transcribe_time": 2.5,
    "translate_time": 1.2
  }
}
```

#### ğŸ“ ç´”æ–‡å­—ç¿»è­¯
**ç«¯é»**ï¼š`POST /text_translate`

ç›´æ¥ç¿»è­¯å·²æœ‰çš„æ–‡å­—å…§å®¹ï¼Œå¿«é€Ÿç²å¾—å¤šèªè¨€ç‰ˆæœ¬ã€‚

```http
POST /text_translate
Content-Type: application/x-www-form-urlencoded

text: è¦ç¿»è­¯çš„æ–‡å­—
language: ä¾†æºèªè¨€ (zh|en|de)
```

#### âš¡ å³æ™‚ä¸²æµç¿»è­¯ï¼ˆSSEï¼‰
**ç«¯é»**ï¼š`POST/GET /sse_audio_translate`

æ”¯æ´å³æ™‚éŸ³é »è™•ç†ï¼Œé©ç”¨æ–¼ç·šä¸Šæœƒè­°ã€ç›´æ’­ç­‰éœ€è¦å³æ™‚å›é¥‹çš„å ´æ™¯ã€‚

```http
# æäº¤éŸ³é »åˆ°è™•ç†ä½‡åˆ—
POST /sse_audio_translate

# å»ºç«‹ Server-Sent Events é€£ç·šæ¥æ”¶çµæœ
GET /sse_audio_translate
Accept: text/event-stream

# åœæ­¢ä¸²æµé€£ç·š
POST /stop_sse
```

#### âš™ï¸ ç³»çµ±ç®¡ç†
**ç«¯é»**ï¼šå¤šå€‹ç®¡ç†ç«¯é»

æä¾›æ¨¡å‹åˆ‡æ›ã€åƒæ•¸èª¿æ•´ã€ç³»çµ±ç‹€æ…‹æŸ¥è©¢ç­‰ç®¡ç†åŠŸèƒ½ã€‚

```http
GET /get_current_model          # æŸ¥çœ‹ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹
GET /list_optional_items        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹å’Œç¿»è­¯é¸é …
POST /change_transcription_model # åˆ‡æ›èªéŸ³è­˜åˆ¥æ¨¡å‹
POST /change_translation_method  # åˆ‡æ›ç¿»è­¯å¼•æ“
POST /set_prompt                # è¨­å®šè‡ªå®šç¾©æç¤ºè©
```

## âš™ï¸ ç³»çµ±é…ç½®

### ğŸŒ æ”¯æ´èªè¨€

<table>
<tr>
<th>èªè¨€ä»£ç¢¼</th>
<th>èªè¨€åç¨±</th>
<th>èªªæ˜</th>
</tr>
<tr>
<td><code>zh</code></td>
<td>ğŸ‡¹ğŸ‡¼ ç¹é«”ä¸­æ–‡</td>
<td>å°ç£åœ°å€æ¨™æº–ä¸­æ–‡</td>
</tr>
<tr>
<td><code>en</code></td>
<td>ğŸ‡ºğŸ‡¸ è‹±æ–‡</td>
<td>ç¾å¼è‹±èª</td>
</tr>
<tr>
<td><code>de</code></td>
<td>ğŸ‡©ğŸ‡ª å¾·æ–‡</td>
<td>æ¨™æº–å¾·èª</td>
</tr>
</table>

### ğŸ™ï¸ è½‰éŒ„æ¨¡å‹é¸é …

<table>
<tr>
<th>æ¨¡å‹åç¨±</th>
<th>èªªæ˜</th>
<th>æ¨è–¦ä½¿ç”¨</th>
</tr>
<tr>
<td><code>large_v2</code></td>
<td>OpenAI Whisper Large v2ï¼ˆé è¨­ï¼‰</td>
<td>âœ… ç©©å®šæ€§ä½³</td>
</tr>
<tr>
<td><code>large_v3</code></td>
<td>OpenAI Whisper Large v3</td>
<td>ğŸ¯ ç²¾åº¦æ›´é«˜</td>
</tr>
<tr>
<td><code>turbo</code></td>
<td>OpenAI Whisper Large v3 Turbo</td>
<td>âš¡ é€Ÿåº¦å„ªå…ˆ</td>
</tr>
<tr>
<td><code>TCM</code></td>
<td>è‡ªå®šç¾©æ¨¡å‹è·¯å¾‘</td>
<td>ğŸ”§ å®¢è£½åŒ–éœ€æ±‚</td>
</tr>
</table>

### ğŸ¤– ç¿»è­¯å¼•æ“é¸é …

<table>
<tr>
<th>å¼•æ“åç¨±</th>
<th>èªªæ˜</th>
<th>éƒ¨ç½²æ–¹å¼</th>
<th>ç‰¹è‰²</th>
</tr>
<tr>
<td><code>gpt4o</code></td>
<td>GPT-4oï¼ˆé è¨­ï¼‰</td>
<td>â˜ï¸ Azure OpenAI API</td>
<td>ğŸ† æœ€é«˜å“è³ª</td>
</tr>
<tr>
<td><code>gemma4b</code></td>
<td>Google Gemma 4B</td>
<td>ğŸ’» æœ¬åœ°é‹è¡Œ</td>
<td>ğŸ”’ éš±ç§ä¿è­·</td>
</tr>
<tr>
<td><code>ollama-gemma</code></td>
<td>Ollama Gemma</td>
<td>ğŸ³ å®¹å™¨éƒ¨ç½²</td>
<td>âš¡ å¿«é€Ÿéƒ¨ç½²</td>
</tr>
<tr>
<td><code>ollama-qwen</code></td>
<td>Ollama Qwen</td>
<td>ğŸ³ å®¹å™¨éƒ¨ç½²</td>
<td>ğŸŒ ä¸­æ–‡å„ªåŒ–</td>
</tr>
</table>

### ğŸ”§ ç’°å¢ƒè®Šæ•¸è¨­å®š

```bash
# ğŸ¤— Hugging Face Tokenï¼ˆGemma æ¨¡å‹ä½¿ç”¨ï¼‰
# è«‹å…ˆè¨ªå•ä¸¦åŒæ„ä½¿ç”¨æ¢æ¬¾ï¼šhttps://huggingface.co/google/gemma-3-4b-it
export HUGGINGFACE_HUB_TOKEN="your_hf_token"

# ğŸ”¥ GPU è¨­å®š
export NVIDIA_VISIBLE_DEVICES=all
export CUDA_VISIBLE_DEVICES=0
```

## ğŸ”§ é€²éšåŠŸèƒ½

### ğŸ¯ å¤šé‡ç­–ç•¥è½‰éŒ„

<table>
<tr>
<th>ç­–ç•¥</th>
<th>æè¿°</th>
<th>é©ç”¨å ´æ™¯</th>
</tr>
<tr>
<td>ğŸ”¹ <strong>ç­–ç•¥ 1</strong></td>
<td>è‡ªå®šç¾©æç¤ºè© + å‰æ–‡èªå¢ƒ</td>
<td>é€£çºŒå°è©±ã€æœƒè­°è¨˜éŒ„</td>
</tr>
<tr>
<td>ğŸ”¸ <strong>ç­–ç•¥ 2</strong></td>
<td>åƒ…ä½¿ç”¨è‡ªå®šç¾©æç¤ºè©</td>
<td>å°ˆæ¥­è¡“èªã€ç‰¹å®šé ˜åŸŸ</td>
</tr>
<tr>
<td>ğŸ”¹ <strong>ç­–ç•¥ 3</strong></td>
<td>ç„¡æç¤ºè©ï¼Œä½æº«åº¦æ¡æ¨£</td>
<td>ä¸€èˆ¬èªéŸ³ã€é«˜æº–ç¢ºåº¦éœ€æ±‚</td>
</tr>
<tr>
<td>ğŸ”¸ <strong>ç­–ç•¥ 4</strong></td>
<td>é«˜æº«åº¦å¤šæ¨£åŒ–æ¡æ¨£</td>
<td>ä¸æ¸…æ™°éŸ³é »ã€å£éŸ³è¼ƒé‡</td>
</tr>
</table>

### ğŸ› ï¸ æ™ºèƒ½å¾Œè™•ç†

<table>
<tr>
<td width="50%">

#### ğŸ” **éŒ¯èª¤æª¢æ¸¬**
- ğŸ”¤ ASR å¸¸è¦‹éŒ¯èª¤è‡ªå‹•è­˜åˆ¥
- ğŸ¢ å“ç‰Œåç¨±æ¨™æº–åŒ–è™•ç†
- ğŸ“ èªæ³•éŒ¯èª¤æ™ºèƒ½ä¿®æ­£
- âš ï¸ å¹»è¦ºå…§å®¹æª¢æ¸¬éæ¿¾

</td>
<td width="50%">

#### âš¡ **æ•ˆèƒ½å„ªåŒ–**
- ğŸ§¹ 24å°æ™‚è‡ªå‹•æ¸…ç†éŸ³é »æª”æ¡ˆ
- ğŸ’¾ GPU è¨˜æ†¶é«”æ™ºèƒ½å›æ”¶
- ğŸ”„ æ¨¡å‹ç†±åˆ‡æ›é›¶ä¸­æ–·
- ğŸ“Š å³æ™‚æ€§èƒ½ç›£æ§ (RTF)

</td>
</tr>
</table>

## ğŸ¯ ä½¿ç”¨ç¯„ä¾‹

### Python å®¢æˆ¶ç«¯ç¯„ä¾‹
```python
import requests

# éŸ³é »ç¿»è­¯
files = {'file': open('audio.wav', 'rb')}
data = {
    'meeting_id': '001',
    'device_id': 'mic_01', 
    'audio_uid': 'audio_001',
    'times': '2024-01-01T10:00:00',
    'o_lang': 'zh',
    'multi_strategy_transcription': 2
}

response = requests.post(
    'http://localhost:80/translate',
    files=files,
    data=data
)
print(response.json())

# æ–‡å­—ç¿»è­¯
data = {'text': 'ä½ å¥½ä¸–ç•Œ', 'language': 'zh'}
response = requests.post(
    'http://localhost:80/text_translate',
    data=data
)
print(response.json())
```

### curl ç¯„ä¾‹
```bash
# éŸ³é »ç¿»è­¯
curl -X POST "http://localhost:80/translate" \
  -F "file=@audio.wav" \
  -F "meeting_id=001" \
  -F "device_id=mic_01" \
  -F "audio_uid=audio_001" \
  -F "times=2024-01-01T10:00:00" \
  -F "o_lang=zh"

# æ–‡å­—ç¿»è­¯  
curl -X POST "http://localhost:80/text_translate" \
  -F "text=Hello World" \
  -F "language=en"
```

## ğŸ“Š æ•ˆèƒ½æœ€ä½³åŒ–

### ğŸ’» å»ºè­°ç¡¬é«”é…ç½®

<table>
<tr>
<th>çµ„ä»¶</th>
<th>æœ€ä½éœ€æ±‚</th>
<th>å»ºè­°é…ç½®</th>
<th>æœ€ä½³æ•ˆèƒ½</th>
</tr>
<tr>
<td><strong>ğŸ”¥ GPU</strong></td>
<td>GTX 1660 (6GB)</td>
<td>RTX 3080 (10GB)</td>
<td>RTX 4090 (24GB)</td>
</tr>
<tr>
<td><strong>ğŸ’¾ è¨˜æ†¶é«”</strong></td>
<td>8GB RAM</td>
<td>16GB RAM</td>
<td>32GB+ RAM</td>
</tr>
<tr>
<td><strong>ğŸ’¿ å„²å­˜</strong></td>
<td>HDD</td>
<td>SATA SSD</td>
<td>NVMe SSD</td>
</tr>
<tr>
<td><strong>ğŸŒ ç¶²è·¯</strong></td>
<td>10 Mbps</td>
<td>100 Mbps</td>
<td>1 Gbps</td>
</tr>
</table>

### âš™ï¸ æ•ˆèƒ½èª¿æ ¡åƒæ•¸

<details>
<summary><strong>ğŸ”§ åœ¨ constant.py ä¸­èª¿æ•´ä»¥ä¸‹åƒæ•¸</strong></summary>

```python
# â±ï¸ è½‰éŒ„è¶…æ™‚è¨­å®š
WAITING_TIME = 60           # å–®ä½ï¼šç§’ï¼Œå»ºè­° 30-120

# ğŸ¯ ç­–ç•¥æ•¸é‡è¨­å®š
MAX_NUM_STRATEGIES = 4      # æœ€å¤§ 4 ç¨®ï¼Œå¯é™ä½è‡³ 1-2 æå‡é€Ÿåº¦

# ğŸ”‡ éœéŸ³å¡«å……
SILENCE_PADDING = True      # æå‡é‚Šç•Œè©è­˜åˆ¥ï¼Œè¼•å¾®å¢åŠ è™•ç†æ™‚é–“

# ğŸ“ˆ å³æ™‚ä¿‚æ•¸è¨ˆç®—
RTF = True                  # å•Ÿç”¨æ•ˆèƒ½ç›£æ§ï¼Œè¼•å¾®å½±éŸ¿æ•ˆèƒ½
```

</details>

### ğŸ“ˆ æ•ˆèƒ½ç›£æ§æŒ‡æ¨™

<table>
<tr>
<td><strong>ğŸ¯ RTF (Real-Time Factor)</strong></td>
<td>< 0.3 å„ªç§€ | 0.3-0.5 è‰¯å¥½ | > 0.5 éœ€å„ªåŒ–</td>
</tr>
<tr>
<td><strong>ğŸ’¾ GPU è¨˜æ†¶é«”ä½¿ç”¨ç‡</strong></td>
<td>< 80% å®‰å…¨ | 80-90% æ³¨æ„ | > 90% å±éšª</td>
</tr>
<tr>
<td><strong>âš¡ è™•ç†é€Ÿåº¦</strong></td>
<td>1åˆ†é˜éŸ³é » < 20ç§’è™•ç†ç‚ºä½³</td>
</tr>
</table>

## ğŸ›¡ï¸ å®‰å…¨æ€§è€ƒæ…®

### API å®‰å…¨
- å»ºè­°ä½¿ç”¨åå‘ä»£ç†ï¼ˆnginxï¼‰
- è¨­å®š API é€Ÿç‡é™åˆ¶
- ç”Ÿç”¢ç’°å¢ƒå•Ÿç”¨ HTTPS

### è³‡æ–™éš±ç§
- éŸ³é »æª”æ¡ˆè‡ªå‹•æ¸…ç†
- æ•æ„Ÿè³‡è¨Šæ—¥èªŒé®è”½
- æ”¯æ´æœ¬åœ°éƒ¨ç½²ï¼Œè³‡æ–™ä¸å‡ºå¢ƒ

## ğŸ› æ•…éšœæ’é™¤

### â“ å¸¸è¦‹å•é¡Œ

<details>
<summary><strong>ğŸ”¥ å•é¡Œ 1ï¼šGPU è¨˜æ†¶é«”ä¸è¶³</strong></summary>

**ç—‡ç‹€**ï¼šCUDA out of memory éŒ¯èª¤

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# ğŸ” æª¢æŸ¥ GPU ä½¿ç”¨ç‹€æ³
nvidia-smi

# ğŸ”§ è§£æ±ºæ–¹æ¡ˆ
# 1. ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹
POST /change_transcription_model
model_name: turbo

# 2. æ¸…ç† GPU å¿«å–
docker restart babelon

# 3. é™ä½ä¸¦ç™¼è™•ç†æ•¸é‡
```

</details>

<details>
<summary><strong>ğŸ¤— å•é¡Œ 2ï¼šæ¨¡å‹è¼‰å…¥å¤±æ•—</strong></summary>

**ç—‡ç‹€**ï¼šHugging Face æ¨¡å‹ä¸‹è¼‰å¤±æ•—

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# ğŸ” æª¢æŸ¥ç™»å…¥ç‹€æ…‹
huggingface-cli whoami

# ğŸ§¹ æ¸…é™¤å¿«å–é‡æ–°ä¸‹è¼‰
rm -rf ~/.cache/huggingface/

# ğŸ”„ é‡æ–°ç™»å…¥
huggingface-cli login --token your_token
```

</details>

<details>
<summary><strong>â˜ï¸ å•é¡Œ 3ï¼šç¿»è­¯ API éŒ¯èª¤</strong></summary>

**ç—‡ç‹€**ï¼šç¿»è­¯åŠŸèƒ½ç„¡å›æ‡‰æˆ–éŒ¯èª¤

**è§£æ±ºæ–¹æ¡ˆ**ï¼š
```bash
# âš™ï¸ æª¢æŸ¥ Azure é…ç½®
cat lib/azure_config.yaml

# ğŸ” é©—è­‰ API é€£é€šæ€§
curl -X POST "https://your-endpoint.openai.azure.com/openai/deployments/your-deployment/chat/completions?api-version=2024-02-15-preview" \
  -H "Content-Type: application/json" \
  -H "api-key: your-api-key" \
  -d '{"messages":[{"role":"user","content":"test"}]}'

# ğŸ“‹ æŸ¥çœ‹è©³ç´°æ—¥èªŒ
tail -f logs/app.log
```

</details>

### ğŸ“‹ é™¤éŒ¯å·¥å…·

<table>
<tr>
<th>å·¥å…·</th>
<th>ç”¨é€”</th>
<th>æŒ‡ä»¤</th>
</tr>
<tr>
<td><strong>ğŸ“Š å³æ™‚æ—¥èªŒ</strong></td>
<td>ç›£æ§ç³»çµ±é‹è¡Œç‹€æ…‹</td>
<td><code>tail -f logs/app.log</code></td>
</tr>
<tr>
<td><strong>ğŸ” éŒ¯èª¤æœå°‹</strong></td>
<td>å¿«é€Ÿå®šä½éŒ¯èª¤è¨Šæ¯</td>
<td><code>grep "error" logs/app.log</code></td>
</tr>
<tr>
<td><strong>ğŸ”§ é™¤éŒ¯æ¨¡å¼</strong></td>
<td>é–‹å•Ÿè©³ç´°æ—¥èªŒ</td>
<td>åœ¨ main.py ä¸­è¨­å®š <code>logging.DEBUG</code></td>
</tr>
<tr>
<td><strong>ğŸ”¥ GPU ç›£æ§</strong></td>
<td>æª¢æŸ¥ GPU ä½¿ç”¨ç‹€æ³</td>
<td><code>watch -n 1 nvidia-smi</code></td>
</tr>
</table>

## ğŸ¤ é–‹ç™¼åƒèˆ‡

### é–‹ç™¼ç’°å¢ƒè¨­å®š
```bash
# è¤‡è£½é …ç›®
git clone https://github.com/bobo0303/Babelon-Translate-Service.git
cd Babelon-Translate-Service

# å®‰è£ä¾è³´
pip install -r requirements.txt

# é…ç½®å¿…è¦æª”æ¡ˆ
# 1. å‰µå»º azure_config.yaml
# 2. æº–å‚™ threading_api.pyï¼ˆåŒ…å«æ©Ÿå¯†è³‡è¨Šï¼‰
# 3. è¨­å®š Hugging Face Token
```

### é–‹ç™¼æ³¨æ„äº‹é …
- è«‹ç¢ºä¿å·²é…ç½® Azure OpenAI API
- GPU ç’°å¢ƒå»ºè­°ä½¿ç”¨ Docker éƒ¨ç½²
- æ¸¬è©¦å‰è«‹ç¢ºèªæ‰€æœ‰ä¾è³´æ¨¡å‹å·²ä¸‹è¼‰
- æ©Ÿå¯†æª”æ¡ˆè«‹å‹¿æäº¤åˆ° repository

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

æœ¬é …ç›®æ¡ç”¨ [MIT License](LICENSE) é–‹æºæˆæ¬Š

## ğŸ“ è¯çµ¡è³‡è¨Š

<table>
<tr>
<td>ğŸ‘¤ <strong>é …ç›®ç¶­è­·è€…</strong></td>
<td>Bobo</td>
</tr>
<tr>
<td>ğŸ› <strong>å•é¡Œå›å ±</strong></td>
<td><a href="https://github.com/bobo0303/Babelon-Translate-Service/issues">GitHub Issues</a></td>
</tr>
<tr>
<td>â­ <strong>å¦‚æœè¦ºå¾—æœ‰ç”¨</strong></td>
<td>æ­¡è¿çµ¦å€‹ Star â­</td>
</tr>
</table>

## ğŸ”„ æ›´æ–°æ—¥èªŒ

### ğŸ“… v1.0.0 (2024-01-01)
- ğŸ‰ **åˆå§‹ç‰ˆæœ¬ç™¼å¸ƒ**
- ğŸµ æ”¯æ´å¤šèªè¨€éŸ³é »è½‰éŒ„ç¿»è­¯
- ğŸ¤– æ•´åˆå¤šç¨® AI æ¨¡å‹ (Whisper, Gemma, Ollama, GPT-4o)
- âš¡ å¯¦ç¾ SSE å³æ™‚ä¸²æµç¿»è­¯
- ğŸ³ Docker å®¹å™¨åŒ–éƒ¨ç½²æ”¯æ´

---

<div align="center">

### ğŸŒŸ æ„Ÿè¬ä½¿ç”¨ Babelon ç¿»è­¯æœå‹™ï¼

**âš ï¸ å…è²¬è²æ˜**ï¼šæœ¬æœå‹™ä»åœ¨æŒçºŒé–‹ç™¼ä¸­ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½æœƒæœ‰èª¿æ•´ã€‚  
ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨å‰è«‹å……åˆ†æ¸¬è©¦ï¼Œä¸¦éµå®ˆç›¸é—œ AI æœå‹™çš„ä½¿ç”¨æ¢æ¬¾ã€‚

<br>

[![Made with â¤ï¸](https://img.shields.io/badge/Made%20with-â¤ï¸-red.svg)](https://github.com/bobo0303/Babelon-Translate-Service)
[![Powered by FastAPI](https://img.shields.io/badge/Powered%20by-FastAPI-009688.svg)](https://fastapi.tiangolo.com)
[![AI Translation](https://img.shields.io/badge/AI-Translation-blue.svg)](https://github.com/bobo0303/Babelon-Translate-Service)

</div>