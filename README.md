# Babelon ç¿»è­¯æœå‹™

## ğŸ“ é …ç›®æ¦‚è¿°

Babelon æ˜¯ä¸€å€‹åŸºæ–¼ FastAPI çš„å¤šèªè¨€éŸ³é »è½‰éŒ„èˆ‡ç¿»è­¯æœå‹™ï¼Œæ•´åˆäº†å…ˆé€²çš„ ASRï¼ˆè‡ªå‹•èªéŸ³è­˜åˆ¥ï¼‰å’Œ AI ç¿»è­¯æŠ€è¡“ã€‚è©²æœå‹™èƒ½å¤ å°‡éŸ³é »æ–‡ä»¶è½‰éŒ„ç‚ºæ–‡å­—ï¼Œä¸¦ç¿»è­¯æˆå¤šç¨®èªè¨€ï¼Œç‰¹åˆ¥é©ç”¨æ–¼æœƒè­°è¨˜éŒ„ã€å³æ™‚ç¿»è­¯ç­‰å ´æ™¯ã€‚

## âœ¨ ä¸»è¦åŠŸèƒ½

### ğŸ¯ æ ¸å¿ƒåŠŸèƒ½
- **éŸ³é »è½‰éŒ„**ï¼šæ”¯æ´å¤šç¨® Whisper æ¨¡å‹ï¼ˆlarge-v2, large-v3, turboï¼‰
- **å¤šèªè¨€ç¿»è­¯**ï¼šæ”¯æ´ä¸­æ–‡ï¼ˆç¹é«”ï¼‰ã€è‹±æ–‡ã€å¾·æ–‡ä¹‹é–“çš„äº’ç›¸ç¿»è­¯
- **å³æ™‚ä¸²æµ**ï¼šæ”¯æ´ Server-Sent Events (SSE) å³æ™‚ç¿»è­¯
- **å¤šé‡ç­–ç•¥è½‰éŒ„**ï¼šæœ€å¤š 4 ç¨®ç­–ç•¥æå‡è½‰éŒ„æº–ç¢ºåº¦
- **æ™ºèƒ½å¾Œè™•ç†**ï¼šè‡ªå‹•ä¿®æ­£ ASR å¸¸è¦‹éŒ¯èª¤

### ğŸ›  æŠ€è¡“ç‰¹è‰²
- **å¤šæ¨¡å‹æ”¯æ´**ï¼šæ•´åˆ OpenAI Whisperã€Gemmaã€Ollamaã€GPT-4o
- **GPU åŠ é€Ÿ**ï¼šæ”¯æ´ CUDA åŠ é€Ÿé‹ç®—
- **å®¹å™¨åŒ–éƒ¨ç½²**ï¼šDocker + Docker Compose å¿«é€Ÿéƒ¨ç½²
- **è³‡æºç®¡ç†**ï¼šè‡ªå‹•æ¸…ç†èˆŠéŸ³é »æ–‡ä»¶ï¼Œè¨˜æ†¶é«”å„ªåŒ–
- **éŒ¯èª¤é‡è©¦**ï¼šå¤šç­–ç•¥å®¹éŒ¯æ©Ÿåˆ¶

## ğŸ— ç³»çµ±æ¶æ§‹

```
Babelon/
â”œâ”€â”€ main.py                 # FastAPI ä¸»æ‡‰ç”¨ç¨‹å¼
â”œâ”€â”€ api/                    # API æ¨¡çµ„
â”‚   â”œâ”€â”€ model.py           # æ ¸å¿ƒæ¨¡å‹ç®¡ç†
â”‚   â”œâ”€â”€ gemma_translate.py # Gemma ç¿»è­¯å¼•æ“
â”‚   â”œâ”€â”€ gpt_translate.py   # GPT-4o ç¿»è­¯å¼•æ“
â”‚   â”œâ”€â”€ ollama_translate.py # Ollama ç¿»è­¯å¼•æ“
â”‚   â”œâ”€â”€ post_process.py    # å¾Œè™•ç†æ¨¡çµ„
â”‚   â””â”€â”€ threading_api.py   # å¤šåŸ·è¡Œç·’ APIï¼ˆéœ€è‡ªè¡Œæº–å‚™ï¼Œå«æ©Ÿå¯†è³‡è¨Šï¼‰
â”œâ”€â”€ lib/                   # å…±ç”¨å‡½å¼åº«
â”‚   â”œâ”€â”€ base_object.py     # åŸºç¤ç‰©ä»¶å®šç¾©
â”‚   â”œâ”€â”€ constant.py        # å¸¸æ•¸èˆ‡è¨­å®š
â”‚   â””â”€â”€ azure_config.yaml  # Azure è¨­å®š
â”œâ”€â”€ tools/                 # å·¥å…·ç¨‹å¼
â”‚   â””â”€â”€ audio_splitter.py  # éŸ³é »åˆ†å‰²å·¥å…·
â”œâ”€â”€ audio/                 # éŸ³é »æª”æ¡ˆæš«å­˜
â””â”€â”€ logs/                  # æ—¥èªŒæª”æ¡ˆ
```

## ğŸš€ å¿«é€Ÿé–‹å§‹

### ç’°å¢ƒéœ€æ±‚
- Python 3.9+
- CUDA æ”¯æ´çš„ GPUï¼ˆå»ºè­°ï¼‰
- Docker & Docker Compose
- 4GB+ GPU è¨˜æ†¶é«”

### å®‰è£éƒ¨ç½²

#### æ–¹æ³•ä¸€ï¼šDocker éƒ¨ç½²ï¼ˆæ¨è–¦ï¼‰
```bash
# è¤‡è£½é …ç›®
git clone <https://github.com/bobo0303/Babelon-Translate-Service.git>
cd Babelon

# å•Ÿå‹•æœå‹™
docker build -t babelon .
    or
docker-compose up -d

# é€²å…¥å®¹å™¨
docker exec -it babelon bash

# åœ¨å®¹å™¨å…§å•Ÿå‹•æœå‹™
python main.py
```

#### æ–¹æ³•äºŒï¼šæœ¬åœ°å®‰è£
```bash
# å®‰è£ä¾è³´
pip install -r requirements.txt

# è¨­å®šç’°å¢ƒè®Šæ•¸
export HUGGINGFACE_HUB_TOKEN="your-hf-token"

# å•Ÿå‹•æœå‹™
python main.py
```

### åˆæ¬¡ä½¿ç”¨è¨­å®š

âš ï¸ **é‡è¦æé†’**ï¼š`azure_config.yaml` éœ€è¦è‡ªè¡Œæº–å‚™ä¸¦é…ç½® Azure OpenAI APIã€‚

1. **azure_config.yaml**ï¼ˆæª”æ¡ˆå¿…å‚™å…§å®¹ï¼‰
```bash
API_KEY:  
AZURE_API_VERSION: 
AZURE_ENDPOINT: 
AZURE_DEPLOYMENT: 
```

2. **Hugging Face ç™»å…¥**ï¼ˆä½¿ç”¨ Gemma æ¨¡å‹éœ€è¦ï¼‰
```bash
huggingface-cli login --token your_hf_token
```

3. **Ollama è¨­å®š**ï¼ˆä½¿ç”¨ Ollama éœ€è¦ï¼‰
```bash
# å»ºç½® ollama docker 
docker run -d -it --gpus all --shm-size 32G --runtime nvidia --device=/dev/nvidia-uvm --device=/dev/nvidia-uvm-tools --device=/dev/nvidiactl --device=/dev/nvidia0 -v ./ollama:/root/.ollama -p 52013:11434 --name ollama ollama/ollama
# å•Ÿå‹• Ollama æ¸¬è©¦
docker exec -it ollama ollama run gemma3:12b-it-qat --verbose
```

## ğŸ“‹ API æ–‡æª”

### åŸºæœ¬è³‡è¨Š
- **æœå‹™åœ°å€**ï¼š`http://localhost:80`
- **API æ–‡æª”**ï¼š`http://localhost:80/docs`
- **å¥åº·æª¢æŸ¥**ï¼š`GET /`

### ä¸»è¦ API ç«¯é»

#### ğŸµ éŸ³é »è½‰éŒ„ç¿»è­¯
**ç«¯é»**ï¼š`POST /translate`

å°‡éŸ³é »æª”æ¡ˆé€²è¡ŒèªéŸ³è­˜åˆ¥ä¸¦ç¿»è­¯æˆå¤šåœ‹èªè¨€ï¼Œæ”¯æ´æœƒè­°è¨˜éŒ„ã€èªéŸ³å‚™å¿˜éŒ„ç­‰æ‡‰ç”¨å ´æ™¯ã€‚

```http
POST /translate
Content-Type: multipart/form-data

file: audio file (.wav, .mp3, .m4a)
meeting_id: string
device_id: string  
audio_uid: string
times: datetime
o_lang: string (zh|en|de)
prev_text: string (å¯é¸ï¼Œå‰æ–‡èªå¢ƒ)
multi_strategy_transcription: int (1-4ï¼Œé è¨­1)
transcription_post_processing: bool (é è¨­true)
```

**å›æ‡‰æ ¼å¼**ï¼š
```json
{
  "status": "OK",
  "message": "ç¿»è­¯çµæœæ‘˜è¦",
  "data": {
    "meeting_id": "123",
    "device_id": "456", 
    "ori_lang": "zh",
    "text": {
      "zh": "ä¸­æ–‡ç¿»è­¯",
      "en": "English translation",
      "de": "Deutsche Ãœbersetzung"
    },
    "times": "2024-01-01T10:00:00",
    "audio_uid": "789",
    "transcribe_time": 2.5,
    "translate_time": 1.2
  }
}
```

#### ğŸ“ ç´”æ–‡å­—ç¿»è­¯
**ç«¯é»**ï¼š`POST /text_translate`

ç›´æ¥ç¿»è­¯å·²æœ‰çš„æ–‡å­—å…§å®¹ï¼Œå¿«é€Ÿç²å¾—å¤šèªè¨€ç‰ˆæœ¬ã€‚

```http
POST /text_translate
Content-Type: application/x-www-form-urlencoded

text: è¦ç¿»è­¯çš„æ–‡å­—
language: ä¾†æºèªè¨€ (zh|en|de)
```

#### âš¡ å³æ™‚ä¸²æµç¿»è­¯ï¼ˆSSEï¼‰
**ç«¯é»**ï¼š`POST/GET /sse_audio_translate`

æ”¯æ´å³æ™‚éŸ³é »è™•ç†ï¼Œé©ç”¨æ–¼ç·šä¸Šæœƒè­°ã€ç›´æ’­ç­‰éœ€è¦å³æ™‚å›é¥‹çš„å ´æ™¯ã€‚

```http
# æäº¤éŸ³é »åˆ°è™•ç†ä½‡åˆ—
POST /sse_audio_translate

# å»ºç«‹ Server-Sent Events é€£ç·šæ¥æ”¶çµæœ
GET /sse_audio_translate
Accept: text/event-stream

# åœæ­¢ä¸²æµé€£ç·š
POST /stop_sse
```

#### âš™ï¸ ç³»çµ±ç®¡ç†
**ç«¯é»**ï¼šå¤šå€‹ç®¡ç†ç«¯é»

æä¾›æ¨¡å‹åˆ‡æ›ã€åƒæ•¸èª¿æ•´ã€ç³»çµ±ç‹€æ…‹æŸ¥è©¢ç­‰ç®¡ç†åŠŸèƒ½ã€‚

```http
GET /get_current_model          # æŸ¥çœ‹ç•¶å‰ä½¿ç”¨çš„æ¨¡å‹
GET /list_optional_items        # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹å’Œç¿»è­¯é¸é …
POST /change_transcription_model # åˆ‡æ›èªéŸ³è­˜åˆ¥æ¨¡å‹
POST /change_translation_method  # åˆ‡æ›ç¿»è­¯å¼•æ“
POST /set_prompt                # è¨­å®šè‡ªå®šç¾©æç¤ºè©
```

## âš™ï¸ é…ç½®èªªæ˜

### æ”¯æ´çš„èªè¨€
- `zh`ï¼šç¹é«”ä¸­æ–‡ï¼ˆå°ç£ï¼‰
- `en`ï¼šè‹±æ–‡ï¼ˆç¾å¼ï¼‰
- `de`ï¼šå¾·æ–‡ï¼ˆæ¨™æº–å¾·èªï¼‰

### è½‰éŒ„æ¨¡å‹é¸é …
- `large_v2`ï¼šOpenAI Whisper Large v2ï¼ˆé è¨­ï¼‰
- `large_v3`ï¼šOpenAI Whisper Large v3
- `turbo`ï¼šOpenAI Whisper Large v3 Turbo
- `TCM`ï¼šè‡ªå®šç¾©æ¨¡å‹è·¯å¾‘

### ç¿»è­¯å¼•æ“é¸é …
- `gpt4o`ï¼šGPT-4oï¼ˆé è¨­ï¼Œéœ€è¦ Azure OpenAI APIï¼‰
- `gemma4b`ï¼šGoogle Gemma 4Bï¼ˆæœ¬åœ°é‹è¡Œï¼‰
- `ollama-gemma`ï¼šOllama Gemma æ¨¡å‹
- `ollama-qwen`ï¼šOllama Qwen æ¨¡å‹

### ç’°å¢ƒè®Šæ•¸è¨­å®š
```bash
# Hugging Face Tokenï¼ˆGemma æ¨¡å‹ä½¿ç”¨ï¼‰
HUGGINGFACE_HUB_TOKEN=your_hf_token

# GPU è¨­å®š
NVIDIA_VISIBLE_DEVICES=all
CUDA_VISIBLE_DEVICES=0
```

## ğŸ”§ é€²éšåŠŸèƒ½

### å¤šé‡ç­–ç•¥è½‰éŒ„
ç³»çµ±æ”¯æ´æœ€å¤š 4 ç¨®è½‰éŒ„ç­–ç•¥ä»¥æå‡æº–ç¢ºåº¦ï¼š
1. **ç­–ç•¥ 1**ï¼šä½¿ç”¨è‡ªå®šç¾©æç¤ºè© + å‰æ–‡èªå¢ƒ
2. **ç­–ç•¥ 2**ï¼šåƒ…ä½¿ç”¨è‡ªå®šç¾©æç¤ºè©
3. **ç­–ç•¥ 3**ï¼šç„¡æç¤ºè©ï¼Œä½æº«åº¦æ¡æ¨£
4. **ç­–ç•¥ 4**ï¼šé«˜æº«åº¦å¤šæ¨£åŒ–æ¡æ¨£

### æ™ºèƒ½å¾Œè™•ç†
- è‡ªå‹•ä¿®æ­£ ASR å¸¸è¦‹éŒ¯èª¤
- å“ç‰Œåç¨±æ¨™æº–åŒ–
- èªæ³•éŒ¯èª¤ä¿®æ­£
- å¹»è¦ºæª¢æ¸¬èˆ‡éæ¿¾

### è‡ªå‹•è³‡æºç®¡ç†
- å®šæ™‚æ¸…ç† 24 å°æ™‚å‰çš„éŸ³é »æª”æ¡ˆ
- GPU è¨˜æ†¶é«”è‡ªå‹•å›æ”¶
- æ¨¡å‹ç†±åˆ‡æ›ä¸ä¸­æ–·æœå‹™

## ğŸ¯ ä½¿ç”¨ç¯„ä¾‹

### Python å®¢æˆ¶ç«¯ç¯„ä¾‹
```python
import requests

# éŸ³é »ç¿»è­¯
files = {'file': open('audio.wav', 'rb')}
data = {
    'meeting_id': '001',
    'device_id': 'mic_01', 
    'audio_uid': 'audio_001',
    'times': '2024-01-01T10:00:00',
    'o_lang': 'zh',
    'multi_strategy_transcription': 2
}

response = requests.post(
    'http://localhost:80/translate',
    files=files,
    data=data
)
print(response.json())

# æ–‡å­—ç¿»è­¯
data = {'text': 'ä½ å¥½ä¸–ç•Œ', 'language': 'zh'}
response = requests.post(
    'http://localhost:80/text_translate',
    data=data
)
print(response.json())
```

### curl ç¯„ä¾‹
```bash
# éŸ³é »ç¿»è­¯
curl -X POST "http://localhost:80/translate" \
  -F "file=@audio.wav" \
  -F "meeting_id=001" \
  -F "device_id=mic_01" \
  -F "audio_uid=audio_001" \
  -F "times=2024-01-01T10:00:00" \
  -F "o_lang=zh"

# æ–‡å­—ç¿»è­¯  
curl -X POST "http://localhost:80/text_translate" \
  -F "text=Hello World" \
  -F "language=en"
```

## ğŸ“Š æ•ˆèƒ½æœ€ä½³åŒ–

### å»ºè­°çš„ç¡¬é«”é…ç½®
- **GPU**ï¼šNVIDIA RTX 3080 ä»¥ä¸Š
- **è¨˜æ†¶é«”**ï¼š16GB+ RAM
- **å„²å­˜**ï¼šSSD æ¨è–¦
- **ç¶²è·¯**ï¼šç©©å®šçš„ç¶²è·¯é€£ç·šï¼ˆAPI èª¿ç”¨ï¼‰

### æ•ˆèƒ½èª¿æ ¡
```python
# åœ¨ constant.py ä¸­èª¿æ•´åƒæ•¸
WAITING_TIME = 60           # è½‰éŒ„è¶…æ™‚æ™‚é–“
MAX_NUM_STRATEGIES = 4      # æœ€å¤§ç­–ç•¥æ•¸
SILENCE_PADDING = True      # éœéŸ³å¡«å……
RTF = True                  # è¨ˆç®—å³æ™‚ä¿‚æ•¸
```

## ğŸ›¡ï¸ å®‰å…¨æ€§è€ƒæ…®

### API å®‰å…¨
- å»ºè­°ä½¿ç”¨åå‘ä»£ç†ï¼ˆnginxï¼‰
- è¨­å®š API é€Ÿç‡é™åˆ¶
- ç”Ÿç”¢ç’°å¢ƒå•Ÿç”¨ HTTPS

### è³‡æ–™éš±ç§
- éŸ³é »æª”æ¡ˆè‡ªå‹•æ¸…ç†
- æ•æ„Ÿè³‡è¨Šæ—¥èªŒé®è”½
- æ”¯æ´æœ¬åœ°éƒ¨ç½²ï¼Œè³‡æ–™ä¸å‡ºå¢ƒ

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è¦‹å•é¡Œ

#### 1. GPU è¨˜æ†¶é«”ä¸è¶³
```bash
# æª¢æŸ¥ GPU ä½¿ç”¨ç‹€æ³
nvidia-smi

# é™ä½æ¨¡å‹è¦æ¨¡
# æ”¹ç”¨ turbo æ¨¡å‹æ›¿ä»£ large_v3
```

#### 2. æ¨¡å‹è¼‰å…¥å¤±æ•—
```bash
# æª¢æŸ¥ Hugging Face ç™»å…¥ç‹€æ…‹
huggingface-cli whoami

# æ¸…é™¤æ¨¡å‹å¿«å–
rm -rf ~/.cache/huggingface/
```

#### 3. ç¿»è­¯ API éŒ¯èª¤
```bash
# æª¢æŸ¥ Azure API é‡‘é‘°è¨­å®š
echo $AZURE_OPENAI_API_KEY

# æŸ¥çœ‹è©³ç´°éŒ¯èª¤æ—¥èªŒ
tail -f logs/app.log
```

### æ—¥èªŒé™¤éŒ¯
```bash
# æŸ¥çœ‹å³æ™‚æ—¥èªŒ
tail -f logs/app.log

# æœå°‹ç‰¹å®šéŒ¯èª¤
grep "error" logs/app.log

# èª¿æ•´æ—¥èªŒç­‰ç´šï¼ˆåœ¨ main.py ä¸­ï¼‰
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ¤ é–‹ç™¼åƒèˆ‡

### é–‹ç™¼ç’°å¢ƒè¨­å®š
```bash
# è¤‡è£½é …ç›®
git clone https://github.com/bobo0303/Babelon-Translate-Service.git
cd Babelon-Translate-Service

# å®‰è£ä¾è³´
pip install -r requirements.txt

# é…ç½®å¿…è¦æª”æ¡ˆ
# 1. å‰µå»º azure_config.yaml
# 2. æº–å‚™ threading_api.pyï¼ˆåŒ…å«æ©Ÿå¯†è³‡è¨Šï¼‰
# 3. è¨­å®š Hugging Face Token
```

### é–‹ç™¼æ³¨æ„äº‹é …
- è«‹ç¢ºä¿å·²é…ç½® Azure OpenAI API
- GPU ç’°å¢ƒå»ºè­°ä½¿ç”¨ Docker éƒ¨ç½²
- æ¸¬è©¦å‰è«‹ç¢ºèªæ‰€æœ‰ä¾è³´æ¨¡å‹å·²ä¸‹è¼‰
- æ©Ÿå¯†æª”æ¡ˆè«‹å‹¿æäº¤åˆ° repository

## ğŸ“„ æˆæ¬Šæ¢æ¬¾

æœ¬é …ç›®æ¡ç”¨ [MIT License](LICENSE)

## ğŸ“ è¯çµ¡è³‡è¨Š

- **é …ç›®ç¶­è­·è€…**ï¼š[Bobo]
- **å•é¡Œå›å ±**ï¼š[GitHub Issues]

## ğŸ”„ æ›´æ–°æ—¥èªŒ

### v1.0.0 (2024-01-01)
- åˆå§‹ç‰ˆæœ¬ç™¼å¸ƒ
- æ”¯æ´å¤šèªè¨€éŸ³é »è½‰éŒ„ç¿»è­¯
- æ•´åˆå¤šç¨® AI æ¨¡å‹
- å¯¦ç¾ SSE å³æ™‚ä¸²æµ

---

**æ³¨æ„**ï¼šæœ¬æœå‹™ä»åœ¨æŒçºŒé–‹ç™¼ä¸­ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½æœƒæœ‰èª¿æ•´ã€‚ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨å‰è«‹å……åˆ†æ¸¬è©¦ã€‚